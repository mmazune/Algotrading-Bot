name: Automated Financial Data Pipeline

on:
  # Trigger on push to main branch
  push:
    branches: [ main ]
  
  # Allow manual triggering with custom inputs
  workflow_dispatch:
    inputs:
      symbol_to_fetch:
        description: 'Stock symbol to fetch (e.g., AAPL, MSFT)'
        required: true
        default: 'AAPL'
        type: string
      interval_for_data:
        description: 'Data interval (e.g., 1day, 1week)'
        required: false
        default: '1day'
        type: choice
        options:
          - 1min
          - 5min
          - 15min
          - 30min
          - 1hour
          - 1day
          - 1week
  
  # Schedule to run daily at midnight UTC
  # Note: GitHub Actions schedule runs are fixed, not dynamically determined by the workflow's 'worth'
  schedule:
    - cron: '0 0 * * *'

jobs:
  run_data_pipeline:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout source code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.9
      uses: actions/setup-python@v5
      with:
        python-version: '3.9'
    
    - name: Install required Python packages
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pandas ta
    
    - name: Run enhanced data collection with API rotation
      # Enhanced script with automatic API key rotation for better rate limit management
      run: python automated_data_pipeline_with_rotation.py
      env:
        # Pipeline parameters
        STOCK_SYMBOL: ${{ github.event.inputs.symbol_to_fetch || 'AAPL' }}
        DATA_FETCH_INTERVAL: ${{ github.event.inputs.interval_for_data || '1day' }}
        
        # Single API keys (backward compatibility)
        FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
        TWELVEDATA_API_KEY: ${{ secrets.TWELVEDATA_API_KEY }}
        
        # Multiple API keys for automatic rotation (recommended)
        FINNHUB_API_KEY_1: ${{ secrets.FINNHUB_API_KEY_1 }}
        FINNHUB_API_KEY_2: ${{ secrets.FINNHUB_API_KEY_2 }}
        FINNHUB_API_KEY_3: ${{ secrets.FINNHUB_API_KEY_3 }}
        FINNHUB_API_KEY_4: ${{ secrets.FINNHUB_API_KEY_4 }}
        FINNHUB_API_KEY_5: ${{ secrets.FINNHUB_API_KEY_5 }}
        FINNHUB_API_KEY_6: ${{ secrets.FINNHUB_API_KEY_6 }}
        FINNHUB_API_KEY_7: ${{ secrets.FINNHUB_API_KEY_7 }}
        
        TWELVEDATA_API_KEY_1: ${{ secrets.TWELVEDATA_API_KEY_1 }}
        TWELVEDATA_API_KEY_2: ${{ secrets.TWELVEDATA_API_KEY_2 }}
        TWELVEDATA_API_KEY_3: ${{ secrets.TWELVEDATA_API_KEY_3 }}
        TWELVEDATA_API_KEY_4: ${{ secrets.TWELVEDATA_API_KEY_4 }}
        TWELVEDATA_API_KEY_5: ${{ secrets.TWELVEDATA_API_KEY_5 }}
        TWELVEDATA_API_KEY_6: ${{ secrets.TWELVEDATA_API_KEY_6 }}
        TWELVEDATA_API_KEY_7: ${{ secrets.TWELVEDATA_API_KEY_7 }}
        
        # Storage credentials (optional)
        MINIO_ACCESS_KEY: ${{ secrets.MINIO_ACCESS_KEY }}
        MINIO_SECRET_KEY: ${{ secrets.MINIO_SECRET_KEY }}
        MINIO_ENDPOINT: ${{ secrets.MINIO_ENDPOINT }}
    
    - name: Verify data file existence and content
      if: success()
      run: |
        # Assuming the Python script saves data to transformed_financial_data.csv
        # Change this filename if your script uses a different output file
        ls -lh transformed_financial_data.csv
        [ -s transformed_financial_data.csv ]
      # This step provides basic verification and could be expanded for more rigorous checks
      # (e.g., checking data rows, specific column presence)
    
    - name: Commit and push transformed data (optional)
      if: success()
      run: |
        git config user.name "GitHub Actions Bot"
        git config user.email "actions@github.com"
        git add transformed_financial_data.csv
        git commit -m "Automated: Update transformed financial data" || echo "No changes to commit"
        git push || echo "No new commits to push"
      # This step commits data directly to the repo and should be used cautiously
      # for large or frequently changing datasets. Consider external storage solutions
      # (like cloud storage) as alternatives for such cases.
